# Projet de Pr√©diction¬†√âlectorale¬†‚Äì POC

> Travail r√©alis√© dans le cadre de la MSPR ¬´‚ÄØData Lake & IA‚ÄØ¬ª. Pour le cahier des charges d√©taill√©, consultez le [sujet](Subject.md).

---

## ‚ú® Objectif

Mettre en place une **preuve de concept** pour **Elexxion**, jeune pousse sp√©cialis√©e dans le conseil √©lectoral, capable de‚ÄØ:

1. Collecter puis normaliser des donn√©es publiques (INSEE, data.gouv, etc.) ou **g√©n√©rer des donn√©es factices** pour acc√©l√©rer les tests.
2. Croiser ces donn√©es avec les r√©sultats √©lectoraux historiques.
3. Entra√Æner un mod√®le supervis√© afin de **pr√©dire le bloc vainqueur** √†¬†+1,¬†+2 et¬†+3¬†ans.
4. Visualiser indicateurs, corr√©lations et projections futures.

---

## üóÇÔ∏è Arborescence du d√©p√¥t

```text
.
‚îú‚îÄ‚îÄ assets/                # diagrammes, jeu de communes, g√©ojson‚Ä¶
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ architecture.png
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ architecture2.png
‚îÇ¬†¬† ‚îî‚îÄ‚îÄ communes.csv
‚îú‚îÄ‚îÄ init-scripts/          # scripts SQL pour provisionner PostgreSQL
‚îú‚îÄ‚îÄ src/
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ ingestion/         # loaders + orchestration (mode API ou Fake)
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ transformation/    # transformers & fusion
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ prediction/        # entra√Ænement + pr√©dicteur
‚îÇ¬†¬† ‚îî‚îÄ‚îÄ visualization/     # graphiques et cartes
‚îú‚îÄ‚îÄ notebooks/             # analyses exploratoires
‚îú‚îÄ‚îÄ test/                  # tests unitaires (pytest)
‚îú‚îÄ‚îÄ config.yml             # active le mode `api` ou `fake`
‚îú‚îÄ‚îÄ main.py                # CLI d‚Äôorchestration (ingest ‚Üí train ‚Üí predict‚Ä¶)
‚îú‚îÄ‚îÄ docker-compose.yml     # services¬†: PostgreSQL, MinIO, Kafka‚Ä¶
‚îî‚îÄ‚îÄ requirements.txt       # d√©pendances Python
```

> **Nouveaut√©**¬†: la CLI expose le switch `--mode fake|api` pour alterner entre donn√©es factices et r√©elles.

---

## üîÑ Pipeline de traitement

1. Avec Dataiku DSS,

![Architecture du Projet](assets/architecture.png)
  
2. Avec des scripts Python  

```mermaid
graph LR
    subgraph 1[Ingestion]
        A1["¬´¬†Loaders¬†¬ª '(elections, socio_eco‚Ä¶)'"] -->|Parquet| A2[data/raw]
    end

    subgraph 2[Transformation]
        A2 --> B1[Transformers]
        B1 -->|Parquet| B2[data/curated]
    end

    subgraph 3[Mod√©lisation]
        B2 --> C1[Training Pipeline]
        C1 -->|joblib| C2[models/]
        C2 --> C3[Predictor]
    end

    subgraph 4[Visualisation]
        B2 --> D1[Heatmaps/Maps]
        C3 --> D2[Projections +1/+2/+3]
    end
```

---

## ‚öôÔ∏è Lancer le projet

### 1. Pr√©requis

* Docker ‚â•¬†24 (pour la stack optionnelle Kafka¬†/ MinIO¬†/ PostgreSQL)
* Python¬†3.11 (ou cr√©ez un¬†venv)

### 2. Installation rapide

```bash
# clonage
$ git clone <url-du-repo>
$ cd mspr1_elexxion

# d√©pendances
$ python3 -m venv .venv && source .venv/bin/activate
$ pip install -r requirements.txt

# services¬†‚Äì optionnels
$ docker-compose up -d   # d√©tachable
```

### 3. Ex√©cution du pipeline

Mode **donn√©es factices** (le plus rapide pour tester l‚Äôencha√Ænement complet)¬†:

```bash
$ python main.py --step all --mode fake
```

Vous devriez obtenir¬†:

```
‚úÖ elections¬†‚Äì 5‚ÄØ000¬†lignes
‚úÖ socio_eco¬†‚Äì 36‚ÄØ000¬†lignes
‚úÖ communes¬† ‚Äì 12‚ÄØ000¬†lignes
‚úÖ Transformation termin√©e : (36‚ÄØ000, ‚Ä¶)
Accuracy¬†: 0.81
```

Mode **API**¬†(pousse les loaders vers les sources r√©elles d√©clar√©es dans `config.yml`)¬†:

```bash
$ python3 main.py --step ingest --mode api  # puis --step transform, train‚Ä¶
```

### 4. Tests unitaires

```bash
$ pytest -q   # ingestion, transformation, pr√©diction
```

---

## üõ†Ô∏è Pile technologique

| Couche              | Outils principaux                       |
| ------------------- | --------------------------------------- |
| ETL                 | Python¬†3.11¬†¬∑ Pandas¬†¬∑ Faker¬†¬∑ Requests |
| Stockage            | Parquet (Datalake MinIO)¬†¬∑ PostgreSQL   |
| Mod√©lisation        | Scikit‚Äëlearn (RandomForest)             |
| Viz                 | Matplotlib¬†¬∑ Geopandas                  |
| Orchestration       | Docker¬†¬∑ docker‚Äëcompose                 |
| Streaming optionnel | Apache¬†Kafka                            |

---

## ü§ù Contribution
=======
# Projet de Pr√©diction √âlectorale - POC

> Ce projet est r√©alis√© dans le cadre d'une MSPR. Pour plus de d√©tails sur les objectifs et les exigences du projet, consultez le [sujet d√©taill√©](Subject.md).

## üìã Description du Projet

Preuve de concept (POC) d√©velopp√©e pour Elexxion, une start-up sp√©cialis√©e dans le conseil √©lectoral. L'objectif est de cr√©er un mod√®le d'intelligence artificielle capable de pr√©dire les tendances √©lectorales en se basant sur des donn√©es socio-√©conomiques, de s√©curit√© et d'emploi.

## üèóÔ∏è Structure du Projet

```shell
.
‚îú‚îÄ‚îÄ notebooks/           # Notebooks Jupyter pour l'analyse exploratoire
‚îú‚îÄ‚îÄ src/                 # Code source du projet
‚îÇ   ‚îú‚îÄ‚îÄ etl/             # Scripts de collecte et transformation des donn√©es
‚îÇ   ‚îú‚îÄ‚îÄ models/          # Mod√®les de pr√©diction
‚îÇ   ‚îî‚îÄ‚îÄ visualization/   # Outils de visualisation
‚îú‚îÄ‚îÄ init-scripts/        # Scripts d'initialisation de la base de donn√©es
‚îî‚îÄ‚îÄ docker-compose.yml   # Configuration Docker
```

## üìä Architecture du Projet

- ![Architecture du Projet](assets/architecture.png)
  
```mermaid
graph TB
    subgraph Infrastructure["üèóÔ∏è Infrastructure"]
        D[("Docker üê≥")]
        KFK["Apache Kafka"]
        MINIO["MinIO Datalake"]
        PSQL["PostgreSQL"]
    end

    subgraph DataProcessing["üìä Traitement des Donn√©es"]
        SCRP["Web Scraping üï∑Ô∏è"]
        ETL["ETL Scripts üîÑ"]
        DSS["Dataiku DSS"]
        NB["Notebooks Jupyter üìì"]
    end

    subgraph PythonStack["üêç Stack Python"]
        PY["Python 3.x"]
        PY --> PD["Pandas & NumPy"]
        PY --> SKL["Scikit-learn"]
        PY --> MPL["Matplotlib"]
    end

    subgraph ProjectModules["üì¶ Modules du Projet"]
        direction TB
        SRC["src/"]
        SRC --> ETLM["etl/"]
        SRC --> MOD["models/"]
        SRC --> VIS["visualization/"]
    end

    %% Flux de donn√©es
    SCRP --> KFK
    KFK --> ETL
    ETL --> MINIO
    MINIO --> DSS
    DSS --> PSQL
    NB --> PSQL
    ETLM --> PY
    MOD --> SKL
    VIS --> MPL

    style Infrastructure fill:#e1f5fe,stroke:#01579b
    style DataProcessing fill:#f3e5f5,stroke:#4a148c
    style PythonStack fill:#e8f5e9,stroke:#1b5e20
    style ProjectModules fill:#fff3e0,stroke:#e65100
```

## üõ†Ô∏è Technologies Utilis√©es

- **Python 3.x** avec les biblioth√®ques :
  - Pandas & NumPy : Manipulation des donn√©es
  - Scikit-learn : Mod√©lisation pr√©dictive
  - Matplotlib : Visualisation
- **Infrastructure Data :**
  - Apache Kafka : Streaming de donn√©es en temps r√©el
  - MinIO : Stockage d'objets S3-compatible
  - PostgreSQL : Base de donn√©es relationnelle
- **Outils de Data Science :**
  - Dataiku DSS : Plateforme de Data Science
  - Kafka UI : Interface de gestion Kafka
- **Docker** : Conteneurisation et orchestration des services
- **SQL** : Requ√™tage et analyse des donn√©es

## üöÄ Installation et D√©marrage

1. **Cloner le repository** :

   ```bash
   git clone <url-du-repo>
   cd <nom-du-repo>
   ```

2. **Installer les d√©pendances** :

   ```bash
   pip install -r requirements.txt
   ```

3. **Lancer les conteneurs Docker** :

   ```bash
   docker-compose up -d
   ```

4. **Ex√©cuter le projet** :

   ```bash
   python main.py
   ```

## üìä Utilisation  

Le projet se compose de trois parties principales :

1. **Collecte et Transformation des Donn√©es** :
   - Scripts Python dans `src/etl/` pour collecter et transformer les donn√©es.
   - Utilisation de Docker pour la gestion des bases de donn√©es.
2. **Mod√©lisation Pr√©dictive** :
   - Scripts Python dans `src/models/` pour la cr√©ation et l'entra√Ænement des mod√®les.
3. **Visualisation des R√©sultats** :
   - Scripts Python dans `src/visualization/` pour g√©n√©rer des visualisations.
  
## ü§ù Contribution  

Ce projet est d√©velopp√© dans le cadre d'un travail d'√©quipe de 4-5 √©tudiants. Pour contribuer :

1. Forkez le repository.
2. Cr√©ez une nouvelle branche : `git checkout -b feature/nom-de-la-fonctionnalit√©`.
3. Faites vos modifications et commit : `git commit -m 'Ajout de la fonctionnalit√© X'`.
4. Poussez vers la branche : `git push origin feature/nom-de-la-fonctionnalit√©`.
5. Ouvrez une Pull Request.


---

## Licence

Ce d√©p√¥t est fourni dans le cadre d‚Äôun exercice acad√©mique‚ÄØ; la licence par d√©faut est **MIT**.

